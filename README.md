# ğŸµ Wave2Notes - AI-Powered Music Transcription

[![Flutter](https://img.shields.io/badge/Flutter-02569B?style=for-the-badge&logo=flutter&logoColor=white)](https://flutter.dev/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://tensorflow.org/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org/)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-FFD21E?style=for-the-badge)](https://huggingface.co/)

> ğŸ“ **Bachelor's Thesis Project** - Automatic Music Transcription using Deep Learning

Transform piano recordings into MIDI files using an Automatic Music Transcription model! Wave2Notes combines advanced deep learning with an intuitive mobile interface to provide real-time music transcription capabilities.

## ğŸŒŸ Features

### ğŸ¹ **Core Functionality**
- **ğŸ¤ Audio Recording** - Record piano music directly in the app
- **ğŸ“ File Upload** - Support for WAV, MP3, M4A, and other audio formats
- **ğŸ¤– AI Transcription** - Deep learning-powered note detection
- **ğŸ¼ MIDI Export** - Professional MIDI files for music software
- **ğŸ“Š Detailed Analysis** - Note timing, pitch, and velocity information

### ğŸ“± **Mobile Experience**
- **ğŸ¨ Piano Roll Visualization** - Interactive note display
- **â–¶ï¸ MIDI Playback** - Built-in audio player
- **ğŸ“¤ Share & Export** - Easy file sharing capabilities
- **ğŸ” User Authentication** - Secure Firebase-based login
- **â˜ï¸ Cloud Storage** - AWS S3 integration for recordings

### ğŸ§  **AI Technology**
- **ğŸ—ï¸ Multi-Task Architecture** - Simultaneous onset, frame, offset, and velocity detection
- **ğŸ¯ Piano-Optimized** - Specialized for 88-key piano transcription
- **âš¡ Real-Time Processing** - Fast inference with GPU acceleration
- **ğŸšï¸ Precision Control** - Configurable detection thresholds

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚    â”‚                   â”‚    â”‚                     â”‚
â”‚ ğŸ“± Flutter App â”‚â”€â”€â”€â–¶â”‚ ğŸŒ Flask API     â”‚â”€â”€â”€â–¶â”‚ ğŸ§  TensorFlow      â”‚
â”‚                 â”‚    â”‚  (Hugging Face)   â”‚    â”‚  Model              â”‚
â”‚  â€¢ Recording    â”‚    â”‚                   â”‚    â”‚                     â”‚
â”‚  â€¢ Playback     â”‚    â”‚  â€¢ Audio Process  â”‚    â”‚  â€¢ CNN Features     â”‚
â”‚  â€¢ UI/UX        â”‚    â”‚  â€¢ ML Inference   â”‚    â”‚  â€¢ BiLSTM+Attention â”‚
â”‚                 â”‚    â”‚  â€¢ MIDI Creation  â”‚    â”‚  â€¢ Multi-task Heads â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                         â”‚
         â”‚                        â”‚                         â”‚
         â–¼                        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚    â”‚                   â”‚    â”‚                     â”‚
â”‚  ğŸ”¥ Firebase    â”‚    â”‚  ğŸ“¦ AWS S3       â”‚    â”‚  ğŸ¤— Hugging Face    â”‚
â”‚                 â”‚    â”‚                   â”‚    â”‚                     â”‚
â”‚ â€¢ Authentication|    â”‚ â€¢ File Storage    â”‚    â”‚  â€¢ Model Hosting    â”‚
â”‚ â€¢ User Data     â”‚    â”‚ â€¢ Recording Backup|    â”‚  â€¢ Version Control  â”‚
â”‚                 â”‚    â”‚                   â”‚    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ Technical Specifications

### ğŸ¯ **Machine Learning Model**

| Component | Details |
|-----------|---------|
| **ğŸ—ï¸ Architecture** | Multi-Task CNN + Bidirectional LSTM with Attention |
| **ğŸ“Š Input** | Mel-spectrogram (229 frequency bins, 32ms resolution) |
| **ğŸ¹ Output** | 88-key piano roll (A0 to C8) |
| **ğŸ¯ Tasks** | Onset Detection, Frame Detection, Offset Detection, Velocity Estimation |
| **ğŸ“ˆ Features** | Residual connections, Self-attention, Vertical dependency modeling |
| **âš–ï¸ Loss Functions** | Weighted Binary Cross-entropy, Focal Loss |
| **ğŸ“ Metrics** | F1-Score, Precision, Recall |

### ğŸ”§ **Technical Stack**

#### **ğŸ–¥ï¸ Backend (Python)**
```yaml
Framework: Flask 3.0.0
ML Library: TensorFlow 2.15.0
Audio Processing: Librosa 0.10.1
MIDI Generation: Mido 1.3.2
Audio Conversion: Pydub 0.25.1
Model Hosting: Hugging Face Hub
API Hosting: Hugging Face Spaces
```

#### **ğŸ“± Frontend (Flutter)**
```yaml
Framework: Flutter (Dart)
Authentication: Firebase Auth
Storage: AWS S3
HTTP Client: Dio
Audio Recording: flutter_sound
File Handling: file_picker
UI Components: Material Design
```

#### **â˜ï¸ Infrastructure**
```yaml
Model Storage: Hugging Face Hub (~500MB SavedModel)
API Hosting: Hugging Face Spaces (4GB RAM, GPU support)
Authentication: Firebase
File Storage: AWS S3
```

## ğŸš€ Getting Started

### ğŸ“‹ **Prerequisites**

- **ğŸ“± Flutter SDK** (>=3.0.0)
- **ğŸ¯ Dart** (>=3.0.0)
- **ğŸ”¥ Firebase Account** (for authentication)
- **â˜ï¸ AWS Account** (for S3 storage)
- **ğŸ“± Android Studio** or **ğŸ Xcode** (for mobile development)

## ğŸ”— **API Integration**

The backend API is hosted on Hugging Face Spaces.

## ğŸ“Š **Model Performance**

### **ğŸ¯ Evaluation Metrics**
- **ğŸµ Note Detection Accuracy**: ~50-55%
- **â±ï¸ Onset Timing Precision**: Â±32ms
- **ğŸ¹ Piano Range Coverage**: 88 keys (A0-C8)
- **âš¡ Processing Speed**: ~10-30 seconds per minute of audio
- **ğŸšï¸ Velocity Estimation**: Correlation coefficient ~0.78

### **ğŸ“ˆ Training Details**
- **ğŸ“š Dataset**: Piano audio recordings with MIDI ground truth
- **ğŸ”„ Training Epochs**: 100+ with early stopping
- **ğŸ“Š Batch Size**: 16
- **ğŸ¯ Optimizer**: Adam with learning rate scheduling
- **ğŸ‹ï¸ Loss Weighting**: Balanced for onset/frame/offset tasks

## ğŸ§ª **Usage Examples**

### **ğŸ“± Mobile App Usage**
1. **ğŸš€ Launch** the Wave2Notes app
2. **ğŸ” Login** with your Firebase account
3. **ğŸ¤ Record** piano music or **ğŸ“ upload** an audio file
4. **â³ Wait** for AI processing (10-30 seconds)
5. **ğŸ“¥ Download** your MIDI file and **ğŸ‘€ view** detected notes in a Synthesia-like format
6. **ğŸµ Play** back the transcription or **ğŸ“¤ share** results

## ğŸ“ **Academic Context**

This project was developed as a **Bachelor's Thesis** on:

**"Automatic Music Transcription using Deep Learning Techniques"**

### **ğŸ¯ Research Objectives**
- Investigate multi-task learning for music transcription
- Compare CNN vs. RNN architectures for temporal modeling
- Evaluate attention mechanisms in music signal processing
- Develop practical mobile application for real-world usage
